---
title: "Optimising tests of efficacy in external pilot trials using Bayesian statistical decision theory"
author: "D. T. Wilson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(pso)
require(mco)
require(RColorBrewer)
require(rgenoud)
require(numDeriv)
require(gridExtra)
require(fastGHQuad)
require(microbenchmark)
require(reshape2)
require(gganimate)
require(xtable)
cols <- brewer.pal(8, "Dark2")
```


## Introduction

Pilot trials are common in complex intervention development, and used to help decide if a confirmatory trial should proceed. Pilot methodology has warned against assessing the efficacy of the intervention in the pilot, primarily because such an assessment (if taking the form of a standard hypothesis test) will have low power to detect an effect of interest, and is therefore likely to incorrectly conclude the intervention is not promising and terminate its development.

The conclusion that a test of efficacy in a pilot will have low power rests on some assumptions. In particular, it assumes that the effect size of interest is of a similar magnitude as in the main trial. This may not be the case if the endpoint in the pilot is different to that in the main trial (as is common in phase II trials); or if we can argue that the idealised setting of the pilot should lead to a larger effect than will be seen in a pragmatic setting. We will take these assumptions as true - the endpoint will be shared, and we cannot justify that the pilot setting will lead to higher effects.

A further assumption is that the type I error rate, or $\alpha$, used in the pilot test will be set at the usual rates of 0.01 - 0.05. Under this restriction, a typical pilot trial as illustrated in Figure \ref{} will indeed have low power, in this case around 21\% for a one-sided test with $\alpha = 0.05$. Having considered this low power unacceptable, by instead deciding to not test at all we are effectively choosing the extreme point on the OC curve with $\alpha = 1, \beta = 0$. This decision reflects an infinite preference for minimising type II over type I errors in pilot trials. Even if the preference is substantial, any finite level would lead to a different choice. For example, consider the point on the OC curve with $\alpha = 0.9, \beta = 0.007$. A 0.1 decrease in type I error at the cost of a 0.007 increase in type II error would be hard to argue against.

If we agree that a type I error rate of 1 is never optimal, how can we find what OCs are optimal? One approach is to view the pilot trial, and subsequent main trial, from the perspective of Bayesian statistical decision theory. If we can define a suitable utility function, and prior distributions on all unknown parameters, we can find the optimal decision (i.e. the best choice or error rates in both trials) by maximising expected utility.

## Problem

The specific problem we will focus on involves choosing the type I and II error rates for a pilot trial, denoted $\alpha_1, \beta_1$, and the subsequent confirmatory trial, denoted $\alpha_2, \beta_2$. Both trials will randomise participants equally to a control and an intervention arm, and will conduct a z-test comparing the group means. The primary endpoint is normally distributed with known variance but with an unknown difference in group expectations, with this uncertainty encpasulated in a normal prior distribution.

We will consider two versions of this problem:
* Find the optimal values for $(\alpha_1, \beta_1, \alpha_2, \beta_2)$;
* Find the optimal $(\alpha_1, \beta_1)$, constraning the main trial at conventional $\alpha_2 = 0.025, \beta_2 = 0.2$.

To provide a direct comparison in each case, we will also consider designs with the pilot OCs set at the current conventional levels of $\alpha_1 = 1, \beta_1 = 0$. Note that this special case will correspond to the optimisation of a confirmatory trial, which may be a useful result in its own right.


## Maximising expected utility

In what follows we will focus initially on the special case where only the confimatory trial is to be designed, before elaborating to the case of a pilot and confirmatory trial together.

### Attributes and value

We construct a utility function by first defining a _value_ function. A value function $v(y)$ should encode our prferences on some attributes $y$ under conditions of certainty, in the sense that $v(y) > v(v')$ iff we prefer $y$ to $y'$. We first need to define the attributes we are interested in and the values they can take. We consider the following:

* the sample size, $n$ 
* cost incurred by changing the current standrd treatment, denoted $C \in \{-,+\}$
* change in the average outcome, denoted $\mu$

We want to define a function $v(n, C, \mu)$ such that

$$(n, C, \mu) \prec (n', C', \mu') \Leftrightarrow v(n, C, \mu) < v(n', C', \mu'),$$

where $y \prec y'$ means we prefer $y$ to $y'$. To construct the value function we first assume that each pair of attributes is preferentially independant of the remaining attribute - that is, the tradeoffs between any two attributes when the remaining attribute is kept fixed do not depend on the level it is fixed at. Then, our preferences can be characterised by a value function of the form 

$$ v(n, C, \delta) = k_n v_n(n) + k_c v_c(C) + k_d v_3(\mu),$$
where

a. $v_i(worst) = 0, v_i(best) = 1, i = 1,2,3$;
b. $0 < \lambda_i < 1, i = 1,2,3$;
c. $\sum_{i=1}^3 \lambda_i = 1$.

We now need to define the component value functions $v_n, v_c, v_d$ and the corresponding weights $k_n, k_c, k_d$. First consider the functions. We assume that the cost of sampling is constant, and so $v_n$ is linear, and similarly assume linearity in the value of change in outcome, $v_d$. Attribute $C$ only has two levels. We use identity value functions for all attributes, which will keep the notation simple. So,

$$ v(n, C, \delta) = k_n n + k_c C + k_d \mu,$$

We need to choose the scaling constants which will determine the realtive importance of each attribute. Given that $k_n + k_c + k_d = 1$ (because the scale of the value function is abitrary), we can deduce the parameter values given two sets of equivalence judgements. First, we ask for the change in outcome $\bar{d}$ that would be needed for us to judge that

$$(n = 0, d = 0, C = +) \sim (n = 100, d = \bar{d}, C = +)$$.

That is, what change in outcome would we want to see to justify an increase in sample size from 0 to 100? Secondly, we ask for the change in outcome $\hat{d}$ that would be needed to judge that

$$(n, d = 0, C = -) \sim (n, d = \hat{d}, C = +).$$

That is, what change in outcome would justify the changing of the standard treatment from the control to the new treatment.

We then have the following values for the scaling parameters:

* $k_d = 1/(1 + \hat{d} + \bar{d}/100)$,
* $k_n = -\lambda_d \bar{d}/100$,
* $k_c = \lambda_d \hat{d}$.

In our example we will take $\bar{d} = 0.01$, meaning that we would be happy to "pay" a sample size of $n = 100$ for a guarenteed increase in the mean outcome of 0.01. We take $\hat{d} = 0.142$, meaning that if we knew the new treatment would improve the mean outcome by 0.142 then we would be indifferent as to whether or not it should be recommended over the control as the standard treatment, bearing in mind the costs associated with implementing such a change. 

(The value 0.142 was in this case arrived at based on the original design which gave a power of 0.8 to detect a difference of 0.2 with a one-sided type I error rate of 0.025. This corresponds to a power of 0.5 at the point $\mu \approx 0.142$, suggesting that this is the true point of indifference or equipoise. This point is described by [@Willan2005] as "the only non-arbitrary point on the power curve"). 

```{r}
get_ks <- function(d_bar, d_hat)
{
  # Get scaling constants
  #k_n <- k_d*val_d(d_bar)
  #k_d <- 1/(1 + val_d(d_bar) + val_d(d_hat))
  #k_c <- k_d*val_d(d_hat)
  
  k_d <- 1/(1 + d_hat - d_bar/100)
  k_n <- -k_d*d_bar/100
  k_c <- 1 - k_d - k_n
  
  return(c(k_d, k_n, k_c))
}

value <- function(mu, n, d_hat, d_bar)
{
  ks <- get_ks(d_bar, d_hat)
  k_d <- ks[1]; k_n <- ks[2]; k_c <- ks[3]
  
  v_pos <- k_d*mu + k_n*n
  v_neg <- 0 + k_n*n + k_c
  
  return(c(v_pos, v_neg))
}
```

### Utility

The value function represents our preferences under conditions of certainty, but in reality we are uncertain about both whether the current standard treatment will be changed (i.e. if the null hypothesis is rejected) and associated change in outcome. We therefore need to defeine a utility function to encode our preferences for distributions defined on our attributes, which will allow us to calculate expected utiliuty and use this to guide our decisions. 

To deduce the form of our utility function, we first argue that we have one attribute (the change in outcome) which is utility independant of the other two attributes. This means that our preferneces for gambles on the change of outcome, with the other attributes kept fixed at some level, does not depend on those levels. So, regardless of how much we have sampled, or whether or not we have incurred the cost of changing the standrad treatment or not, our preferneces for gambles on the change in outcome will be the same. Given this assumption together with the additive form of the value function, the utility function must have one of the following forms:

* $u(n, d, C) = 1 - e^{-\rho v(n, d, C)}, \rho > 0$,
* $u(n, d, C) = v(n, d, C), \rho = 0$.
* $u(n, d, C) = -1 + e^{-\rho v(n, d, C)}, \rho > 0$,

That is, the utlity function over the scaler attribute $V$ must be of the exponential form, which implies constant absolute absolute risk aversion. The coefficient of absolute risk aversion is
$$\frac{-u''(v)}{u'(v)} = \rho.$$
Assessment of $u$ then just requires $\rho$. Becuase $d$ is utility indpendant of $n$ and $C$, we can think about gambles on $d$ whilst ignoring the value of the other attributes. Doing so, we find the point $d^*$ which is the certainty equivalent to a gamble which will give $d = 0$ and $d = 1.5$ each with probability 0.5. If

$d^* = 0.5 * 0 + 0.5 * 1.5 = 0.75$$

then $\rho = 0$ and the utlity function is additive. If the left-hand side is is greater then $\rho > 0$ (risk averse), and if the right hand side is greater, $\rho < 0$ (risk neutral). Either way, we can determine $\rho$ by setting the utilities equal and solving:

$$u(n, d^*, C) = 0.5 u(n, 0, C) + 0.5 u(n, 1.5, C)$$

### MEU for a single confirmatory trial

Before considering the more general problem of jointly designing a pilot and confirmatory trial, we first look at the special case where the pilot OCs are fixed at $\alpha_1 = 1, \beta_1 = 0$ and only the confirmatory trial OCs are to be determined. In this case we can get an exact expression for our expected utility, avoiding any numerical integration. Denote the sample mean in the trial by $x$, and the critical value in the test by $d$. The utility function is denoted by $u(\mu, x)$. Then,

$$
\begin{aligned}
E_{\mu | x} [u(\mu, x)] &= E_{\mu | x}[1 - e^{-\rho(k_d\mu + k_n n) I\{x > d\} -\rho(k_n n + k_c) I\{x < d\}}] \\
&= I\{x > d\} E_{\mu | x}[1 - e^{-\rho(k_d\mu + k_n n)}] + I\{x < d\}E_{\mu | x}[1 - e^{-\rho(k_n n + k_c)}]
\end{aligned}
$$

Since the second term doens't include $\mu$, we can take the expectation operator out. For the first term we have:

$$
E_{\mu | x}[1 - e^{-\rho(k_d\mu + k_n n)}] = 1 - e^{\rho k_n n} E_{\mu | x}e^{-a k_d \mu}]
$$
The expectation here is over the posterior distribution for $\mu$ given $x$, which (assuming $\sigma^2$ is known) is normal by conjugacy, specifically

$$
\mu |x \sim N \left( \mu_1 = \sigma_1^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{xn}{2\sigma^2} \right), ~ \sigma_1^2 = 1/\left( \frac{1}{\sigma_0^2} + \frac{n}{2\sigma^2} \right) \right).
$$

Note that this follows from the normal likelihod for the sample difference, $x | \mu  \sim N(\mu, 2\sigma^2/n)$. It takes the form as a moment generating function. For a normaly distibuted $Y \sim N(a, b^2)$,

$$
E[e^{tY}] = e^{ta + \frac{b^2 t^2}{2}}.
$$
This gives us

$$
E_{\mu | x}[1 - e^{-\rho(k_d\mu + k_n n)}] = 1 - e^{-\rho k_n n} \left(e^{t\mu_1 + \frac{\sigma_1^2 t^2}{2}} \right),
$$

where $t=\rho k_d$. This then all gives an expected utility conditional on $x$ of

$$
E_{\mu | x} [u(\mu, x)] = I\{x > d\} \left[1 - e^{-\rho k_n n} \left(e^{t\mu_1 + \frac{\sigma_1^2 t^2}{2}} \right) \right] + I\{x < d\}\left[1 - e^{-\rho(k_n n + k_c)}\right].
$$

We now need to take the expectation of this over the marginal distribution for $x$.

$$
\begin{aligned}
E[u(\mu, x)] &= E_x\left[ E_{\mu | x} [u(\mu, x)] \right] \\
 &= E_x\left[ I\{x > d\} \left(1 - e^{-\rho k_n n} e^{t\sigma_1^2(\frac{\mu_0}{\sigma_0^2} + \frac{xn}{2\sigma^2}) + \frac{\sigma_1^2 t^2}{2}} \right) \right] + 
 E_x\left[ I\{x < d\}\left(1 - e^{-\rho(k_n n + k_c)}\right) \right].
\end{aligned}
$$

Expanding the first term:

$$
\begin{aligned}
E_x\left[ I\{x > d\} \left(1 - e^{-\rho k_n n} e^{t\sigma_1^2(\frac{\mu_0}{\sigma_0^2} + \frac{xn}{2\sigma^2}) + \frac{\sigma_1^2 t^2}{2}} \right) \right] &= Pr[x > d] \left( 1 - e^{-\rho k_n n} e^{\frac{\sigma_1^2 t^2}{2}} e^{\frac{t\sigma_1^2 \mu_0}{\sigma_0^2}}   \right) E_x \left[ e^{\frac{t\sigma_1^2 x n}{2\sigma^2}} \right].
\end{aligned}
$$

The probability of $x > d$ is just the unconditional power:

$$
Pr[x > d] = 1- \Phi\left(\frac{d-\mu_0}{\sqrt{\sigma_0^2 + \frac{2\sigma^2}{n}}} \right)
$$

For the expectation, we again use a moment generating function. Here, the sample difference $x$ is normally distributed $N(\mu_0, \sigma_x^2 = \sigma^2 + \frac{2\sigma^2}{n})$ but we have conditioned on $x > d$ and so have a truncated normal. The moment generating function for $Y \sim N(a,b^2), Y > d$ is

$$
E[e^{rY}] =  \left( e^{ar + \frac{b^2 r^2}{2} } \right) \left(\frac{1 - \Phi(\frac{d-a}{b} - br)}{1 - \Phi(\frac{d-a}{b} )} \right)
$$

So, for our case we have

$$
 E_x \left[ e^{\frac{t\sigma_1^2 x n}{2\sigma^2}} \right] = \left( e^{\mu_0 r + \frac{\sigma_x^2 r^2}{2} } \right) \left(\frac{1 - \Phi(\frac{d-\mu_0}{\sigma_x} - \sigma_x r)}{1 - \Phi(\frac{d-\mu_0}{\sigma_x})} \right),
$$

where $r = \frac{t \sigma_1^2 n}{2\sigma^2}$. Finally, putting everything together, we get

$$
\begin{aligned}
E[u(\mu, x)] =& E_x\left[ E_{\mu | x} [u(\mu, x)] \right] \\
 =& \left[ 1- \Phi\left(\frac{d-\mu_0}{\sqrt{\sigma_0^2 + \frac{2\sigma^2}{n}}} \right)  \right] \left[ 1 - e^{-\rho k_n n} e^{\frac{\sigma_1^2 t^2}{2}} e^{\frac{t\sigma_1^2 \mu_0}{\sigma_0^2}} \left( e^{\mu_0 r + \frac{\sigma_x^2 r^2}{2} } \right) \left(\frac{1 - \Phi(\frac{d-\mu_0}{\sigma_x} - \sigma_x r)}{1 - \Phi(\frac{d-\mu_0}{\sigma_x})} \right)  \right] + \\
 & \Phi\left(\frac{d-\mu_0}{\sqrt{\sigma_0^2 + \frac{2\sigma^2}{n}}} \right) \left[1 - e^{-\rho(k_n n + k_c)}\right].
\end{aligned}
$$

This is all for the case $\rho > 0$, but the case $\rho < 0$ follows immediately. For the case $\rho = 0$ the utility function is the value function. The expected utility is then

$$
\begin{aligned}
E[u(\mu, x)] =& E_x\left[ E_{\mu | x} [u(\mu, x)] \right] \\
=& E_x \left[ I\{x > d\} (k_d \mu_1 + k_n n) + I\{x \leq d\} (k_n n + k_c) \right] \\
=& k_n n + Pr(x > d) k_d E_{x | x > d} \left[ \mu_1 \right] + Pr(x \leq d)k_c,
\end{aligned}
$$
where $\mu_1$ is, as before, the mean of the posterior distribution on $\mu$ given $x$:
$$
\mu_1 = \sigma_1^2 \left( \frac{\mu_0}{\sigma_0^2} + \frac{xn}{2\sigma^2} \right), ~ \sigma_1^2 = 1/\left( \frac{1}{\sigma_0^2} + \frac{n}{2\sigma^2} \right).
$$
We are then left with calculating the expected value of this posterior mean with respect to the marginal distribution of $x$ conditional on $x > d$. Since $x$ is normally distrubuted with mean $\mu_0$ and variance $\sigma_x^2 = \sigma_0^2 + 2\sigma^2/n$, the conditional distribution is a truncated normal with mean

$$
\mu_0 + \sigma_x \phi\left(\frac{d - \mu_0}{\sigma_x} \right)/\left(1 - \Phi\left(\frac{d - \mu_0}{\sigma_x}\right) \right).
$$

Implementing all this gives the following function for expected utility:

```{r}
exp_u <- function(d, n, d_hat, d_bar, rho, sig, mu_0, sd_0)
{
  k <- get_ks(d_bar, d_hat)
  k_d <- k[1]; k_n <- k[2]; k_c <- k[3]
  
  sd_1 <- sqrt(1/(1/sd_0^2 + n/(2*sig^2)))
  t <- -rho*k_d
  sig_x <- sqrt(sd_0^2 + 2*sig^2/n)
  r <- (t * sd_1^2 *n)/(2*sig^2)
  
  if(rho > 0){
  (1 - pnorm((d-mu_0)/sig_x)) * (1 - exp(-rho*k_n*n) *
                                   exp(sd_1^2 *t*t/2) *
                                   exp(t*sd_1^2 *mu_0/sd_0^2) *
                                   exp(mu_0*r + (sig_x^2*r*r/2)) *
                                   ((1-pnorm((d-mu_0)/sig_x - sig_x*r))/(1-pnorm((d-mu_0)/sig_x)))) +
    pnorm((d-mu_0)/sig_x) * (1 - exp(-rho*(k_n*n + k_c)))
  } else if(rho < 0){
    (1 - pnorm((d-mu_0)/sig_x)) * (-1 + exp(-rho*k_n*n) *
                                   exp(sd_1^2 *t*t/2) *
                                   exp(t*sd_1^2 *mu_0/sd_0^2) *
                                   exp(mu_0*r + (sig_x^2*r*r/2)) *
                                   ((1-pnorm((d-mu_0)/sig_x - sig_x*r))/(1-pnorm((d-mu_0)/sig_x)))) +
    pnorm((d-mu_0)/sig_x) * (-1 + exp(-rho*(k_n*n + k_c)))
  } else {
    al <- (d-mu_0)/sig_x
    z <- 1-pnorm(al)
    m_x_trunc <- mu_0 + sig_x*dnorm(al)/z
    k_n*n + k_d*((sd_1^2)*(mu_0/(sd_0^2) + m_x_trunc*n/(2*sig^2)))*(1 - pnorm((d-mu_0)/sig_x))  + k_c*( pnorm((d-mu_0)/sig_x) )
  }
}
```

We can check the function against a simulation:

```{r, eval=T}
test_u <- function(mu, x, d, rho, mu_0, k)
{
  k_d <- k[1]; k_n <- k[2]; k_c <- k[3]
  go <- x > d
  if(rho > 0){
   1 - exp(-rho*(k_d*mu + k_n*n)*go - rho*(k_n*n + k_c)*(1-go)) 
  } else if(rho < 0){
    -1 + exp(-rho*(k_d*mu + k_n*n)*go - rho*(k_n*n + k_c)*(1-go)) 
  } else {
    (k_d*mu + k_n*n)*go + (k_n*n + k_c)*(1-go)
  }
}

test_sim <- function(sig, d, rho, mu_0, sd_0, k)
{
  mu <- rnorm(1, mu_0, sd_0)
  x <- rnorm(1, mu, sqrt(2*sig^2/n))
  c(mu, x, test_u(mu, x, d, rho, mu_0, k))
}

d_bar <- 0.01; d_hat <- 0.142; mu_0 <- 0; sd_0 <- 0.244949; sig <- 0.25; rho <- 1; n <- 20
k <- get_ks(d_bar, d_hat)

# Simulated expected utility
N <- 1000000
us <- replicate(N, test_sim(sig, d=0.3, rho, mu_0, sd_0, k)[3])
# 98% confidence interval around the MC estimate
c(mean(us) - qnorm(0.99)*sqrt(var(us)/N), mean(us) + qnorm(0.99)*sqrt(var(us)/N))

exp_u(d=0.3, n, d_hat, d_bar, rho, sig, mu_0, sd_0)
```

### MEU for a pilot and confirmatory trial programme

Denote pilot and main sample size by $n_1, n_2$ and repsective sample means $x_1, x_2$. The ciritcal values against which these are compared are $d_1, d_2$. The utility is now

$$
u(\mu, x_1, x_2) = 1 - e^{-\rho(k_d\mu + k_n (n_d1+n_2))I[x_1>d_1, x_2>d_2] -\rho(k_n (n_1+n_2) + k_c)I[x_1 > d_1, x_2 < d_2] - \rho(k_n n_1 + k_c)I[x_1 < d_1]}.
$$

It has not been possible to obtain a closed-form expression for the expectaed utility over the joint distribution of $\mu, x_1, x_2$. However, the expected utility conditional on $\mu$ is

$$
\begin{aligned}
E_{x_1, x_2, | \mu}[u(\mu, x_1, x_2)] =& Pr[x_1>d_1, x_2>d_2 | \mu]\left(1-e^{\rho(k_dv_d(\mu) + k_n v_n(n_1+n_2)}\right) + \\
& Pr[x_1 > d_1, x_2 < d_2 | \mu] \left(1- e^{-\rho(k_n v_n(n_1+n_2) + k_c)} \right) + \\
& Pr[x_1 < d_1 | \mu] \left( 1-e^{-\rho(k_n v_n(n_1) + k_c)} \right).
\end{aligned}
$$

Since the sample means are distributed as $x_i | \mu \sim N(\mu, 2\sigma^2/n_i)$ and are independant conditional on $\mu$, the relavant probabilities are easily calculated.

```{r}
# Condition on mu first
exp_u_mu <- function(mu, n1, d1, n2, d2, k, rho, mu_0, sd_0, sig)
{
  pow1 <- 1-pnorm(d1, mu, sqrt(2*sig^2/n1))
  pow2 <- 1-pnorm(d2, mu, sqrt(2*sig^2/n2))
  
  if(rho > 0){
    pow1*pow2*(1-exp(-rho*(k[1]*mu + k[2]*(n1+n2)))) +
    pow1*(1-pow2)*(1-exp(-rho*(k[2]*(n1+n2) + k[3]))) +
    (1-pow1)*(1-exp(-rho*(k[2]*n1 + k[3])))
  } else if(rho < 0) {
    pow1*pow2*(-1+exp(-rho*(k[1]*mu + k[2]*(n1+n2)))) +
    pow1*(1-pow2)*(-1+exp(-rho*(k[2]*(n1+n2) + k[3]))) +
    (1-pow1)*(-1+exp(-rho*(k[2]*n1 + k[3])))
  } else {
    pow1*pow2*(k[1]*mu + k[2]*(n1+n2)) +
    pow1*(1-pow2)*(k[2]*(n1+n2) + k[3]) +
    (1-pow1)*(k[2]*n1 + k[3])
  }
}

# For example,
d1 <- d2 <- 0.14
n1 <- n2 <- 20
sd_0 <- sig <- 0.25; mu_0 <- 0; rho <- 1

k <- get_ks(0.01, 0.142)

exp_u_mu(0.2, n1, d1, n2, d2, k, rho, mu_0, sd_0, sig)
```

We are then left with integrating out the $\mu$:

$$
E[u(\mu, x_1, x_2)] = \int E_{x_1, x_2, | \mu} [ u(\mu, x_1, x_2)] p(\mu) d\mu, 
$$

where $p(\mu)$ is the normal prior density. Because we are integrating with a normal density weighting function, we can use Gauss-Hermite Quadrature (as implemented in the `fastGHQuad` package) to evaluate the integral efficiently and accurately. Specifically, Gauss-Hermite approximates integrals of the form

$$
\int_{-\infty}^{\infty} e^{-x^2}f(x) dx \approx \sum_{i=1}^{n} w_i f(x_i).
$$

To use when integrating over a normal distribution $y \sim N(\mu, \sigma^2)$, we just use the tranform $y = \sqrt{2}\sigma x + \mu$ so the integration points are now $x_i\sqrt{2}\sigma + \mu$ and the weights are $w_i / \sqrt{\pi}$.

```{r}
# Set up the quadrature points and weights
rule <- gaussHermiteData(100)
rule$x <- rule$x*sqrt(2)*sd_0 + mu_0

exp_u_joint <- function(x, k, rho, mu_0, sd_0, sig, rule)
{
  n1 <- x[1]; d1 <- x[2]; n2 <- x[3]; d2 <- x[4]
  u <- ghQuad(f=exp_u_mu, rule=rule, n1=n1, d1=d1, n2=n2, d2=d2,
         k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig)/sqrt(pi)
  # return expected utility and a penalty to avoid having a larger sample in the
  # pilot than in the confirmatory trial
  return(-u+100*(n2<n1))
}

# For example,
exp_u_joint(c(5, 0.01, 30, 0.12), k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule)

# Compare with MC estimate
mean(replicate(100000, exp_u_mu(rnorm(1, mu_0, sd_0), n1=5, d1=0.01, n2=30, d2=0.12, k, rho, mu_0, sd_0, sig)))

# Compare run times for GH and regual integration
microbenchmark(
#integrate(exp_u_mu, -10, 10, n1=n1, d1=d1, n2=n2, d2=d2,
#         k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig)$value
ghQuad(f=exp_u_mu, rule=rule, n1=n1, d1=d1, n2=n2, d2=d2,
         k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig)
)

# Regular:
#     min      lq     mean  median       uq     max neval
# 331.488 346.986 464.3999 528.411 539.1685 716.582   100

# GH:
#    min     lq     mean median     uq    max neval
# 40.114 41.209 52.13059 43.214 49.596 218.44   100
```

## Illustration

To illustrate the method, we look at the problem described in Stallard (2012). There, the MCID and the outcome standard deviation were both equal to 1, implying a standard phase III sample size of 22 per arm to get 0.9 power with a one-sided type I error rate of 0.025. Taking the same approach as before to setting $\hat{d}$, we find that standard phase III would have had around 50\% power at $\hat{d} = 0.605$. We set $\bar{d} = 0.05$, giving us a similar costs of sampling as our previous example (inflating a a factor of five to relfect the interest in an effect of around 1 rather than 0.2).

```{r, eval=F}
mu_0 <- 0; sd_0 <- 1; sig <- 1
rho <- 1; d_bar <- 0.05; d_hat <- 0.62
k <- get_ks(d_bar, d_hat)
rule <- gaussHermiteData(100)
rule$x <- rule$x*sqrt(2)*sd_0 + mu_0

ptm <- proc.time()
opt <- genoud(exp_u_joint, 4, Domains = matrix(c(0,-40,0,0,300,10,300,10),ncol=2), 
              print.level = 0, boundary.enforcement = 2, 
              k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule)
proc.time() - ptm

x <- opt$par

r1 <- c(x, 
         1 - pnorm(x[2]/sqrt(2*(sig^2)/x[1])),
         pnorm(x[2], 1, sqrt(2*sig^2/x[1])),
         1 - pnorm(x[4]/sqrt(2*(sig^2)/x[3])),
         pnorm(x[4], 1, sqrt(2*sig^2/x[3])))
r1

opt <- genoud(exp_u_joint, 4, Domains = matrix(c(0,-40,0,0, 300,-39,300,10),ncol=2), 
              print.level = 0, boundary.enforcement = 2, 
              k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule)

x <- opt$par

r2 <- c(x, 
         1 - pnorm(x[2]/sqrt(2*(sig^2)/x[1])),
         pnorm(x[2], 1, sqrt(2*sig^2/x[1])),
         1 - pnorm(x[4]/sqrt(2*(sig^2)/x[3])),
         pnorm(x[4], 1, sqrt(2*sig^2/x[3])))

opt <- genoud(exp_u_joint, 4, Domains = matrix(c(0,-40,20.99,0.6, 300,10,21,0.604858),ncol=2), 
              print.level = 0, boundary.enforcement = 2, 
              k=k, rho=rho, mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule)

x <- opt$par

r3 <- c(x, 
         1 - pnorm(x[2]/sqrt(2*(sig^2)/x[1])),
         pnorm(x[2], 1, sqrt(2*sig^2/x[1])),
         1 - pnorm(x[4]/sqrt(2*(sig^2)/x[3])),
         pnorm(x[4], 1, sqrt(2*sig^2/x[3])))
 
tab <- as.data.frame(rbind(r1, r2, r3))

tab <- data.frame(pr = c("Unrestricted", "No pilot test", "Conventional main trial OCs"),
                  n_1=round(tab[,1]), n_2=round(tab[,3]),
                  a_1 = round(tab[,5], 2), p_1 = round(tab[,6], 3),
                  a_2 = round(tab[,7], 3), p_2 = round(tab[,8], 3)
                   )
colnames(tab) <- c("Problem", "$n_1$", "$n_2$", "$\\alpha_1$", "$1 - \\beta_1$", "$\\alpha_2$", "$1 - \\beta_2$")
tab[2,2] <- 0
tab

print(xtable(tab, digits = c(1,0,0,0,2,3,3,3)), booktabs = T, include.rownames = F, 
       sanitize.text.function = function(x) {x}, floating = F,
       file = "./paper/tables/ill.txt")
```


## Evaluation

### A single confirmatory trial

```{r}
eval_scenario <- function(x)
{
  n <- x[1]; d_hat <- x[2]; d_bar <- x[3]; rho <- x[4]; sig <- x[5]; mu_m <- x[6]; mu_sd <- x[7]
  
  # For a given n, find the optimal type I error rate
  opt <- optim(rnorm(1,0,0.1), exp_u, gr=NULL, control=list(fnscale=-1), method="BFGS",
                              n=n, d_hat=d_hat, d_bar=d_bar, rho=rho, sig=sig, mu_0=mu_m, sd_0=mu_sd)
  
  d <- opt$par; u <- opt$value
  alpha <- (n==0)*0 + (n!=0)*(1-pnorm(d*sqrt(n/(2*sig^2))))
  
  # Get conditional power at the original "mcid" of 0.2
  cp <- pnorm(0.2/sqrt(2*sig^2/n) - qnorm(1-alpha)) 
  
  # Get unconditional power
  sig_x <- sqrt(sd_0^2 + 2*sig^2/n)
  up <- 1 - pnorm((d-mu_0)/sig_x)
  
  # Get the posterior probabilities Pr[mu > d_hat | x=d] and Pr[mu > 0 | x=d]
  post_mean <- (mu_m/(mu_sd^2) + d/(2*(sig^2)/n))/(1/(mu_sd^2)+1/(2*(sig^2)/n))
  post_var <- 1/(1/(mu_sd^2)+1/(2*(sig^2)/n))
  # post probs that mu is greater than a) d_hat, and b) 0
  posts <- 1-pnorm(c(d_hat, 0), post_mean, sqrt(post_var))
  
  return(c(d, -u, alpha, cp, up, posts))
}
```

A table of scenarios to optimise:
```{r}
df <- data.frame(n=seq(1, 100, 1))
df$d_hat <- 0.142; df$d_bar <- 0.01; df$rho <- 1
df$sig <- 0.25; df$mu_m <- 0; df$mu_sd <- 0.244949

df <- cbind(df, t(apply(df, 1, eval_scenario)))
names(df)[8:14] <- c("d", "u", "alpha", "cp", "up", "post_d", "post_0")


ggplot(df, aes(n, -u, colour=alpha)) + geom_line() + 
  geom_vline(xintercept = df[which.min(df$u), "n"], colour = cols[1], linetype = 2) +
  theme_minimal() + ylab("Expected utility")
```

The design which gives the maximum expected utility is a sample size of $n = 28$ and a type I error rate of $\alpha = 0.0104$. This gives a power of 0.752 to detect the original MCID of 0.2, and gives an unconditional probability of rejecting the null of 0.275. The null hypothesis will be rejected if the sample mean is greater than 0.1545, or equivalently, if the posterior probability that $\mu > 0.142$ is greater than 0.5112. Note that the lowest expected utility is given by a sample size of $n = 0$. We can include a set-up cost into the value function (in units of sample size) to correct for this, leading to a minimum sample size for the trial to be better than doing nothing at all.

In the specific example considered here (i.e. for this choice of parameter values in the utility function and prior distribution), the optimal type I and II error rates are not all that far off traditional values. We now find optimal designs for a range of scenarios, keeping the prior fixed but varying the parameters $\bar{d}$ and $\rho$ in the utility function. Recall that $\bar{d}$ is a measure of the cost of sampling, with larger values meaning larger costs; and $\rho$ meaures the attitude to risk, with larger values meaning more risk-averse. We keep $\hat{d}$ constant at 0.142.


```{r}
eval_design <- function(des, x)
{
  if(des[1] < 2 | des[1] > 150 | des[2] < 0 | des[2] > 0.5) return(10000)
  n <- des[1]; d <- des[2]
  d_hat <- x[1];  rho <- x[2]; d_bar <- x[3]; sig <- x[4]; mu_m <- x[5]; mu_sd <- x[6]
  u <- exp_u(d, n, d_hat, d_bar, rho, sig, mu_m, mu_sd)
  return(-u)
}
  
get_opt_design <- function(x)
{
  opt <- optim(c(20, 0.1), eval_design, x=x)
  
  n <- opt$par[1]; d <- opt$par[2]
  sig <- x[4]
  alpha <- 1-pnorm(d/sqrt(2*(sig^2)/n))
  cp <- pnorm(0.2/sqrt(2*sig^2/n) - qnorm(1-alpha)) 
  
  # Check "optimal" design is better than a null design with n=0
  u_0 <- eval_design(c(0.001,0.01), x=x)
  if(u_0 < opt$value){
    Print("RRRR")
    alpha <- 0; cp <- 0; n<- 0; d <- 0
  }
  
  return(c(n,d,alpha,cp))
}

df_sen <- expand.grid(rho = seq(5, -5, length.out = 20),
                 d_bar = seq(0.002, 0.02, length.out = 20))

df_sen$d_hat <- 0.142; df_sen$sig <- 0.25; df_sen$mu_m <- 0; df_sen$mu_sd <- 0.244949
df_sen <- df_sen[,c(3,1,2,4:6)]

ptm <- proc.time()
df_sen <- cbind(df_sen, t(apply(df_sen, 1, get_opt_design)))
proc.time() - ptm

names(df_sen)[7:10] <- c("n", "d", "a", "cp")

ggplot(df_sen, aes(a, 1-cp, colour=rho))  + 
  theme_minimal() + 
  geom_rect(aes(xmin = 0.005, xmax = 0.025, ymin = 0.1, ymax = 0.2), fill=cols[1], alpha=0.2) +
  geom_point()


#saveRDS(df_sen, "./data/opt_single.Rda")
```

Plot an animation:
```{r, eval=F}
df2 <- df_sen[,c(2,3,7,9,10)]
df2$n <- df2$n/100
df2 <- melt(df2, id.vars = c("rho", "d_bar"))
names(df2) <- c("rho", "d_bar", "var", "val")

p <- ggplot(df2, aes(d_bar, val, linetype=var)) + geom_line() +
  theme_minimal() + 
  transition_states(rho)

animate(p, nframes = 20, fps = 5)
```

We see that there are some parameter values in the ranges considered which lead to typical operating characteristics of $\alpha = 0.025$ and $\beta \in [0.1, 0.2]$. Specifically, the costs of sampling must be very low and / or the attitude to risk must be very risk-averse. 

```{r}
p1 <- ggplot(df_sen, aes(d_bar, cp, colour=rho, group=rho)) + geom_point() + geom_line() + theme_minimal()
p2 <- ggplot(df_sen, aes(rho, cp, colour=d_bar, group=d_bar)) + geom_point() + geom_line() + theme_minimal()
p3 <- ggplot(df_sen, aes(d_bar, a, colour=rho, group=rho)) + geom_point() + geom_line() + theme_minimal()
p4 <- ggplot(df_sen, aes(rho, a, colour=d_bar, group=d_bar)) + geom_point() + geom_line() + theme_minimal()
p5 <- ggplot(df_sen, aes(d_bar, n, colour=rho, group=rho)) + geom_point() + geom_line() + theme_minimal()
p6 <- ggplot(df_sen, aes(rho, n, colour=d_bar, group=d_bar)) + geom_point() + geom_line() + theme_minimal()
p7 <- ggplot(df_sen, aes(d_bar, d, colour=rho, group=rho)) + geom_point() + geom_line() + theme_minimal()
p8 <- ggplot(df_sen, aes(rho, d, colour=d_bar, group=d_bar)) + geom_point() + geom_line() + theme_minimal()

grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8, ncol=2) + theme_minimal()
```


What can we say from these?
* sample size is almost exclusively detemined by $\bar{d}$, not by the attitude to risk;
* Both parameters influence the optimal cuut-off point, and thus the type I and II error rates;
* As we become more risk averse, the optimal design reduces the type I error rate and increases the type II;
* As the cost of sampling increases, optimal type II error reduces and type I increases.

### A pilot and confirmatory trial programme

We can optimise over the design parameters $n_1, d_1, n_2, d_2$ (which are equivalent to the OCs $\alpha_1, \beta_1, \alpha_2, \beta_2$. In addition to optimising without any contraints, we also consider the case where $n_1$ must be greater than equal to 6. This reflects the scenario where the pilot trial will be going ahead in any case to assess feasibility questions, and will use a standard pilot sample size of 6 (clusters, leading to 30 pareticipants) in each arm.

```{r, eval=F}
# Get optimal programmes for a range of risk aversions
k <- get_ks(0.01, 0.142)

df <- expand.grid(rho = seq(5, -5, length.out = 20),
                 d_bar = seq(0.002, 0.02, length.out = 20))

#df <- data.frame(rho = rev(c(seq(-5,-0.001,0.5), 0.001, seq(0.001,5,0.5))))

rs <- NULL
x <- c(10, 0.001, 10, 0.001)
for(i in 1:nrow(df)){
  k <- get_ks(df$d_bar[i], 0.142)
  #opt <- psoptim(x, exp_u, k=k, rho=df$rho[i], mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule, 
   #                lower = c(0,-2,0,0), upper= c(300,3,300,3), control=list(maxit=1000))
  opt <- genoud(exp_u, 4, Domains = matrix(c(6,-2,0,0,300,3,300,3),ncol=2), print.level = 0, boundary.enforcement = 2, starting.values = x,
                 k=k, rho=df$rho[i], mu_0=mu_0, sd_0=sd_0, sig=sig, rule=rule)
  x <- opt$par

  r <- c(x, 1-pnorm(x[2]/sqrt(2*(sig^2)/x[1])),
    1-pnorm(x[2], 0.2, sqrt(2*sig^2/x[1])),
    1-pnorm(x[4]/sqrt(2*(sig^2)/x[3])),
    1-pnorm(x[4], 0.2, sqrt(2*sig^2/x[3])))
  
  rs <- cbind(rs,r)
}

df <- cbind(df, t(rs))

names(df) <- c("rho", "d_bar", "n1", "d1", "n2", "d2", "a1", "b1", "a2", "b2")

# With no lower bound on pilot n
#saveRDS(df, "./data/opt_joint.Rda")

# With a lower bound of 6 on pilot n
#saveRDS(df, "./data/opt_joint_lb6.Rda")
```

```{r}
df <- readRDS("./data/opt_joint.Rda")
#df <- readRDS("./data/opt_joint_lb6.Rda")

df2 <- df#[df$d_bar == 0.02,]
df2$n1 <- df2$n1/100; df2$n2 <- df2$n2/100; 
df2 <- melt(df2, id.vars = c("rho", "d_bar"))
df2 <- cbind(df2[substr(df2$variable,2,2) == "1",], df2[substr(df2$variable,2,2) == "2",3:4])
names(df2)[3:6] <- c("t1", "v1", "t2", "v2")
df2 <- melt(df2, id.vars = c("rho", "d_bar", "t1", "t2"))
df2 <- df2[,c(1,2,3,5,6)]
df2$t1 <- substr(df2$t1,1,1)
df2$variable <- substr(df2$variable,2,2)
names(df2) <- c("rho", "d_bar", "var", "trial", "val")

p <- ggplot(df2[df2$var != "d",], aes(rho, val, colour=trial, linetype=var)) + geom_line() +
  theme_minimal() +
  scale_colour_manual(values=cols) +
  scale_y_continuous(breaks=seq(0,1,0.1)) + 
  transition_states(d_bar)

animate(p, nframes=20, fps=5)


sub <- df2[df2$d_bar %in% unique(df2$d_bar)[c(1, 4, 8, 12, 16, 18)],]
sub$d_bar <- round(sub$d_bar, 3)

ggplot(sub[sub$var != "d",], aes(rho, val, colour=trial, linetype=var)) + geom_line() +
  theme_minimal() +
  scale_colour_manual(values=cols) +
  #scale_y_continuous(breaks=seq(0,1,0.1)) + 
  facet_wrap(. ~ d_bar, labeller = labeller(d_bar = d_bar_labs)) +
  xlab(expression(rho)) + ylab("Value") +
  labs(colour = "Trial", linetype = "Variable")
  


```






Note that we may be able to obtain derivatives of the expected utility with respect to the four design variables, which could lead to a faster or more reliable optimisation.

In this case, there is not much distinction in the results of the constrained and unconstrained case, so we focus on the latter here. The above plots shows how error rates and sample size of each trial vary with $\rho$, the attitude to risk (where higher $\rho$ means more risk-averse). We see a clear trend that as we become more risk-averse, the type I error rate in both pilot and conformatory trials decreases. Power in the pilot trial deceases with risk-aversion, while it increases in the main trial. The sample size of both trials increases. The above plot animates with varying $\bar{d}$, the cost of sampling (higher value meaning higher cost). As the cost increases, we see lower sample sizes in each trial, lower power, and increased type I error rate. 

Over the ranges of parameters considered here, it may be noted that the type I error rate in the pilot trial is always much higher than 0.025 and much lower than 1, being instead in the range (0.219, 0.595). For the main trial, the largest type I error rate is 0.062, but can be as low as 0.000002. Power, on the other hand, are often not far from the conventional choices of 0.8 and 0.9. In the pilot trial they  ranged from 0.858 to 0.980, while in the main trail the range was (0.791, 0.946).

Regarding the conventional strategy of not testing in the pilot, we don't find any scenarios here where this is optimal. The results do suggest, however, that as we become more risk-seeking we have a higher optimal type I error in the pilot. And indeed  we see that as we go beyond the range considered above, we do find optimal pilot designs where $\alpha_1$ approaches 1 (for example, when $\rho = -10$). Given that we might in general expect decision-makers to be risk-averese or at best risk-neutral, these results both suggest that the current strategy is far from optimal and give some insight into why this is (i.e. not testing is a risky and inefficient strategy, so only attractive when we are seeking risk).

### Comparison

We can contrast our results with Stallards 2012 paper. He assumes that the phase III trial operating characteristics will be fixed, and chooses those of the phase II trial to minimise the expected total sample size per phase III success. If we also fix the phase III trial OCs, how do the methods compare in terms of optimal phase II design?

```{r, eval=F}
compare <- function(y)
{
  n1 <- y[1]; alpha1 <- y[2]
  alpha2 <- 0.025; beta2 <- 0.1; sig <- 0.25; delta <- 0.2; mu_m <- 0; mu_sd <- 0.244949
  n2 <- 33
  
  # expected utility
  d1 <- qnorm(1-alpha1)*sqrt(2*(sig^2)/n1); d2 <- qnorm(1-alpha2)*sqrt(2*(sig^2)/n2)
  x <- c(d1, d2, n1, n2)
  u <- -exp_util_2(x, d_hat=0.142, d_bar=0.1, rho=5, sig=sig, mu_m=mu_m, mu_sd=mu_sd)[1]

  # expected sample size
  exp_ss <- integrate(exp_ss_integrand, -10, 10, sig=sig, n1=n1, alpha1=alpha1, n2=n2, mu_m=mu_m, mu_sd=mu_sd)$value
  
  # probability of a success at phase III
  exp_suc_integrand <- function(mu, sig, n1, alpha1, n2, alpha2, mu_m, mu_sd)
  {
    pow1 <- 1-pnorm(qnorm(1-alpha1)-mu/sqrt(2*(sig^2)/n1))
    pow2 <- 1-pnorm(qnorm(1-alpha2)-mu/sqrt(2*(sig^2)/n2))
    return(pow1*pow2*prior_mu(mu, mu_m, mu_sd))
  }
  exp_suc <- integrate(exp_suc_integrand, -10, 10, sig=sig, n1=n1, alpha1=alpha1, n2=n2, alpha2=alpha2, mu_m=mu_m, mu_sd=mu_sd)$value
  
  return(c(u, exp_ss/exp_suc, exp_ss, exp_suc))
}

grid <- expand.grid(n1 = seq(0,20, 0.5), alpha1 = seq(0, 0.7, 0.05))
grid <- cbind(grid, t(apply(grid, 1, compare)))
grid$beta1 <- apply(grid, 1, function(x) 1-pnorm(qnorm(1-x[2])-1/sqrt(2*(1^2)/x[1])))

grid[c(which.max(grid[,3]), which.min(grid[,4])),]
```

The biggest difference is in the sample size and power - much, much lower using Stallard's objective. Plotting the objective functions:

```{r, eval=F}
ggplot(grid, aes(n1, alpha1, z=V1, colour=..level..)) + geom_contour()
ggplot(grid, aes(n1, alpha1, z=V2, colour=..level..)) + geom_contour()
```

What explains the difference? The Stallard metric seems intuitive - why wouldn't it be optimal to design trials to minimise the expected sample size required per phase III success? First, ignore the phase II trial and apply this metric to a single phase III study. Now, the probability of success is the unconditional power, and if we choose the sample size to maximise the ratio of sample size to this probability 

```{r, eval=F}
df <- expand.grid(n=1:100, p=seq(0,1,0.1))
df$r <- df$n/df$p

ggplot(df, aes(n, p, z=r, colour=..level..)) + geom_contour(bins=50)
```

So, using Stallard's metric, there is no scope to incorporate judegemnts about the costs of sampling, the benefits of the treatment effect, and the attitude to risk. These are going to be very different across settings - for example, if there is a large supply of potential treatments to send to phase II, or if we only have one and may not have another for some time. In the former case we can afford to have lots of small phase IIs (i.e. low power) with high thresholds (i.e. low alpha), knowing that one is very likely to pass and that it will be of good quality. In the latter case we do not want to miss any true effect of our treatment, so we will want high power and high alpha. How do we articulate these in our model? If we have lots of treatments then the "patient horizon" will be short - it won't be long until the decision we reach is overrulled by another trial. But if there are not many treatments (or not many resources available to do a trial) then the horizon could be very long. This corresponds to the value of the tretament effect attribute - in the latter case it is worth more, so our $\bar{d}$ parameter will be lower. Which is eactly what was shown in our sensitivity analysis.

Key argument is that the Stallard metric is too simple. We can plot the possible options in terms of expected sample size and probability of success, and we would expect that the specific design we choose from this admissable set will depend on aspects of the scenario that are not considered in his model. So, the suggestion that we should always choose that from this set that minimises expected sample size over probability of suceess is not sufficiently flexible. Another point is that his metric / model does not take many parameters as arguments. We need the prior distribution for the treatment effect, and the known outcome variance, but that's all - the optimal design does not depend on the MCID, or on sampling costs, or on an attitude to risk. All of which we might informally incorporate into our decision when presented with the expected sample size / probability of success curve.

## Discussion

### Possible extensions

* Non-inferiority trials

Automatic?

* Internal pilots

Recall that when conditioning on mu we had to calculate the probabilities of: both tests passing; pilot test passing but main test failing; and pilot test failing. These are not much harder to compute in the internal pilot setting - the stats at both stages will be distributed as a bivariate normal so can get the probabilities directly from e.g. mvtnorm (whereas before we could calculate the probabilities for each stage using pnorm and then multiply since they were independant, conditional on mu). In terms of reporting the results, we might be more interested in looking at the optimal value of overall type I and II error rates, and less interested in the pilot OCs alone.

* Multi-arm and/or multi-stage trials

See Lee2019, who consider using utility to decide at an iterim analysis if another arm should be added to the trial. Multi-arm here could also consider multiple endpoints, aiming to select one for the primary analysis in the main trial. This would be a significant extension since the different endpoints would have to go into the utility, and the correlation between endpoints within participants would have to be modelled.

* Binary endpoint

We will have a conjugate analysis given a beta prior, in that the posterior distribution for the rate parameter in both arms will remain beta. But our utility will depend on the absolute or relative difference between these (relative being the most appropriate at the popullation level?). Unless this has a nice closed form, are we then looking at a numerical integration over two betas? Is this easy to do?

```{r, eval=F}
ff <- function(p)
{
  (p[1]-p[2])*dbeta(p[1], 2, 4)*dbeta(p[2], 8, 4)
}

# Optimisation involves about 12,000 evaluations at the moment
# Compare the time this would take

ptm <- proc.time()
x <- replicate(12000, hcubature(ff, c(0,0), c(1,1)))
#x <- replicate(12000, exp_u_joint(c(10, 0.5, 20, 0.5), k, rho, mu_0, sd_0, sig, rule))
proc.time() - ptm

# For Beta integration:
# user  system elapsed 
# 564.25    0.89  571.43 

# For normal integration:
# user  system elapsed 
#   0.53    0.05    0.56 
```


### WP3.2

* Incorporating pilot data into main trial design

If the pilot trial has collected data on feasibility aspects such as recruitment, follow-up and adherence rates, we can use the posterior distributions in the design of the main trial. For simple cases where an analytic power function is available, we can continue laregly as above but doing a more general MC integration when calculating expected utility (as opposed to the exact or GH methods used above) using the set of posterior samples generated in the pilot.

When an analytic power function is not available, the expectation can be estimated by sampling trial data and analysis conditional on each posterior sample. This will then add a very significant computational burden, taking us to an expensive optimisation problem which we can solve using EGO.

From a pilot design perspective, we can look at value of information if we can simulate pilot data and for each data set quoickly obtain the subsequent main trial design and its expected utility.

* Testing feasibility in the pilot 

In the above we have focussed on the very simple case with a normal prior on the treatment effect and a known standard deviation. We want to relax this to allow for unknown SDs, particularly in the context of cRCTs (where we still assume an unadjusted t-test as the main decision making tool, but now at the cluster level; so we have the non-central t-distribution which allows for heteroskedasticity and apprioximately allows for random cluster size). What are the implications?

- We don't have a conjugate analysis, but this was only useful when deriving the analytic expression for extected utility for the phase III only case.
- In the pilot / phase III case we had a numerical integration over a normal density, so could use G-H quadrature. Now the integration will be over two or three dimensions and not necessarily normal, so we will need a more general integration method (e.g. `pcubature` or MC - these can be speeded up with vecotrisation, i.e. writing the integrand function in C++).
- Previously, setting up the problem as one of choosing error rates for a z-test was equivalent to choosing n and assuming MEU decision making. But this won't necessarily be the case now, i.e. an MEU decision rule will not necessarily be of the same form as a t-test. 

In addition to looking at what error rates are admissable over our possible utility functions and for some example priors, we can also look at how the pilot design differs to what we might normally do in terms of inflating for cluster size. That is, one concern about testing in CI pilots with clustering is that these will be particually underpowered, even more so than normal, because of the very limited cluster sizes.

- If we are allowing for clustering, this will need to be incorporated into the utility in terms of the sampling costs at both levels. Plenty of previous papers suggesting this approach (although again, not explicitly decision theiretic, with values more than utilities).



As noted above, the simplicity of the problem means we can search for the optimal solution in terms of $\alpha$ and $\beta$ and that this optimum will be the same as if we searched over $n$ and assumed an MEU decision once the data are obtained. But this may not always be the case - that is, it might be impossible to find a frequentist test that is optimal in terms of our utility. To explore this, relax the problem slightly by no longer assuming that the variance is known. We then want to solve two problems, one searching for the $\alpha$ and $\beta$ for a t-test, and one searching for the optimal $n$ for an MEU analysis.

```{r, eval=F}
t_test_power <- function(mu, alpha, n, sig)
{
  # Small-sample t-test
  df <- 2*n-2
  ncp <- sqrt(n)*mu/(sqrt(2)*sig)
  prob <- 1- suppressWarnings( pt(qt((1-alpha), df), df=df, ncp=ncp) )
  
  if(n != 0){
    return(prob)
  } else {
    return(0)
  }
}

prior_sig <- function(sig, sig_m, sig_sd)
{
  sig_m <- 0.25; sig_sd <- 0.03
  sig_var <- sig_sd^2
  a <- (sig_m^2)/sig_var
  b <- sig_m/sig_var
  #hist(rgamma(10000, a, b))
  return(dgamma(sig, a, b))
}

t_test_integrand <- function(x, n, d_hat, d_bar, rho, alpha, mu_m, mu_sd)
{
  mu <- x[1]; sig <- x[2]
  pow <- t_test_power(mu, alpha, n, sig)
  u <- util(mu, n, d_hat, d_bar, rho)
  u_pos <- u[1]; u_neg <- u[2]
  x <- (pow*u_pos + (1-pow)*u_neg)*prior_mu(mu, mu_m, mu_sd)*prior_sig(sig)
  return(x)
}

exp_util_t_test <- function(n, alpha, d_hat, d_bar, rho, sig, mu_m, mu_sd)
{
  u <- hcubature(t_test_integrand, lowerLimit = c(-3, 0.00001), upperLimit =  c(3, 0.6), alpha=alpha, n=n, mu_m=mu_m, mu_sd=mu_sd, d_hat=d_hat, d_bar=d_bar, rho=rho)$integral
  return(-u)
}

eval_design_t_test <- function(des, x)
{
  if(des[1] < 2 | des[1] > 150 | des[2] < 0 | des[2] > 1) return(10000)
  n <- round(des[1]); alpha <- des[2]
  d_hat <- x[1]; d_bar <- x[2]; rho <- x[3];  mu_m <- x[4]; mu_sd <- x[5]
  u <- exp_util_t_test(n, alpha, d_hat, d_bar, rho, sig, mu_m, mu_sd)
  return(u)
}

sim_data <- function(n=NULL, n_max=NULL, mu_m, mu_sd)
{
  if(is.null(n)) n <- round(runif(1,1,n_max))
  sig_m <- 0.25; sig_sd <- 0.03
  sig_var <- sig_sd^2
  a <- (sig_m^2)/sig_var
  b <- sig_m/sig_var
  mu <- rnorm(1, mu_m, mu_sd)
  sig <- rgamma(1, a, b)
  y1 <- rnorm(n, 0, sig)
  y2 <- rnorm(n, mu, sig)
  return(c(mu, n, mean(y2) - mean(y1), sd(c(y1,y2))))
}

eval_design_MEU <- function(n)
{
  d_hat <- x[1]; d_bar <- x[2]; rho <- x[3];  mu_m <- x[4]; mu_sd <- x[5]
  
  N <- 10^6
  
  df <- t(replicate(N, sim_data(n=n, mu_m=mu_m, mu_sd=mu_sd)))
  df <- cbind(df, t(apply(df, 1, function(x, d_hat=0.142, d_bar=0.01, rho=1) util(x[1], x[2], d_hat, d_bar, rho))))
  df <- as.data.frame(df)

  mod_pos <- gam(V5 ~ s(V3) + s(V4), data = df)
  mod_neg <- gam(V6 ~ s(V3) + s(V4) , data = df)
  
  #mod_pos2 <- randomForest(df[,3:4], df[,5])
  
  u <- apply(cbind(predict(mod_pos), predict(mod_neg)), 1, max)
  return(c(-mean(u), var(u)/N))
}
 
get_opt_designs <- function(x)
{
  opt_t_test <- optim(c(20, 0.1), eval_design_t_test, x=x)
  opt_MEU <- optim(20, eval_design_MEU, x=x, mod_pos=mod_pos, mod_neg=mod_neg, method="Brent", lower = 2, upper = 50)
  
  
  #opt <- optim(c(20, 0.1), eval_design, x=x, lower=c(0.001,0), upper=c(150,0.3))
  n <- opt$par[1]; d <- opt$par[2]
  sig <- x[4]
  alpha <- 1-pnorm(d/sqrt(2*(sig^2)/n))
  cp <- power(0.2, alpha, n, sig)
  
  # Check "optimal" design is better than a null design with n=0
  u_0 <- eval_design(c(0.001,0.01), x=x)
  if(u_0 < opt$value){
    Print("RRRR")
    alpha <- 0; cp <- 0; n<- 0; d <- 0
  }
  
  return(c(n,d,alpha,cp))
}

df_sen <- expand.grid(d_bar=seq(0.001, 0.15, length.out = 20), rho=seq(-3,3,length.out = 20))

df_sen$d_hat <- 0.142; df_sen$sig <- 0.25; df_sen$mu_m <- 0; df_sen$mu_sd <- 0.244949
df_sen <- df_sen[,c(3,1,2,4:6)]

df_sen <- cbind(df_sen, t(apply(df_sen, 1, get_opt_design)))
names(df_sen)[7:10] <- c("n", "d", "alpha", "cp")

# Try using a kriging based optimiser
library(DiceOptim)

ns <- seq(10, 40, 15)
us <- sapply(ns, eval_design_MEU)
df <- data.frame(n = ns, u=us[1,], v=us[2,])
mod <- km(design=df[,1,drop=FALSE], response=df[,2], noise.var=df[,3])

pred <- predict.km(mod, newdata = df[,1, drop=FALSE], type="SK")
plot(df[,1], pred$mean)

n <- (5:40)[which.max(sapply(5:40, EQI, model=mod))]
n
u <- eval_design_MEU(n)
df <- rbind(df, c(n, u))
mod <- km(design=df[,1,drop=FALSE], response=df[,2], noise.var=df[,3])

to_plot <- data.frame(n=seq(5,40,0.01))
pred <- predict.km(mod, newdata = to_plot, type="UK")
to_plot$u <- pred$mean; to_plot$lo <- pred$lower95; to_plot$up <- pred$upper95
ggplot(to_plot, aes(n, u)) + geom_ribbon(aes(ymin = lo, ymax = up), fill="grey") +
  geom_line() + geom_point(data=df)

```

## References

## Supplementary material


To assist with optimisation, we can find the derivatives. First, re-write the expected utility as a function of the critical value $d$:
$$
[1-f(d)]\left[1-x\left(\frac{1-g(d)}{1-h(d)}\right)\right] + f(d)(1-y) = 1 - f(d) - x\left(\frac{1-g(d)}{1-h(d)}\right) + f(d)x\left(\frac{1-g(d)}{1-h(d)}\right) + f(d)(1-y).
$$
Its derivative is:
$$
-f'(d) - x\left(\frac{-g'(d)}{1-h(d)} + \frac{(1-g(d))h'(d)}{(1-h(d))^2}\right) + f'(d) x\left(\frac{1-g(d)}{1-h(d)}\right) + f(d)x\left(\frac{-g'(d)}{1-h(d)} + \frac{(1-g(d))h'(d)}{(1-h(d))^2}\right) + f'(d)(1-y),
$$
where
$$
\begin{aligned}
f'(d) &= \phi\left(\frac{d-\mu_0}{\sqrt{\sigma_0^2 + \frac{2\sigma^2}{n}}}\right) \frac{1}{\sqrt{\sigma_0^2 + \frac{2\sigma^2}{n}}} \\
g'(d) &= \phi\left(\frac{d-\mu_0}{\sigma_x} - \sigma_x r\right)\frac{1}{\sigma_x} \\
h'(d) &= \phi\left(\frac{d-\mu_0}{\sigma_x}\right)\frac{1}{\sigma_x}
\end{aligned}
$$
Implementing in R and checking against the numerical gradient:
```{r, eval=F}
require(numDeriv)

f <- function(d){pnorm((d-mu_0)/sqrt(sd_0^2 + 2*sig^2/n))}
g <- function(d){pnorm((d-mu_0)/sig_x - sig_x*r)}
h <- function(d){pnorm((d-mu_0)/sig_x)}
f_d <- function(d){dnorm((d-mu_0)/sqrt(sd_0^2 + 2*sig^2/n))/sqrt(sd_0^2 + 2*sig^2/n)}
g_d <- function(d){dnorm((d-mu_0)/sig_x - sig_x*r)/sig_x}
h_d <- function(d){dnorm((d-mu_0)/sig_x)/sig_x}

d <- 0.14
sd_1 <- sqrt(1/(1/sd_0^2 + n/(2*sig^2)))
t <- -rho*k_d
sig_x <- sqrt(sd_0^2 + 2*sig^2/n)
r <- (t * sd_1^2 *n)/(2*sig^2)
x <- exp(-rho*k_n*n) *
                                 exp(sd_1^2 *t*t/2) *
                                 exp(t*sd_1^2 *mu_0/sd_0^2) *
                                 exp(mu_0*r + (sig_x^2*r*r/2))

d_deriv <- function(d, x, k_n, k_c)
{
  -f_d(d) - x*(-g_d(d)/(1-h(d)) + (1-g(d))*h_d(d)/(1-h(d))^2) + f_d(d)*x*((1-g(d))/(1-h(d))) + f(d)*x*(-g_d(d)/(1-h(d)) + (1-g(d))*h_d(d)/(1-h(d))^2) + f_d(d)*(1 - exp(-rho*(k_n*n + k_c)))
}

d_deriv(0.14, x=x, k_n=k_n, k_c=k_c)
grad(exp_u, 0.14, n=n, d_hat=d_hat, d_bar=d_bar, rho=rho, sig=sig, mu_0=mu_0, sd_0=sd_0)
```
Now try optimising:
```{r, eval=F}
optim(rnorm(1,0,0.1), exp_u, gr=NULL, control=list(fnscale=-1), method="BFGS",
      n=n, d_hat=d_hat, d_bar=d_bar, rho=rho, sig=sig, mu_0=mu_0, sd_0=sd_0)
```

```{r, eval=F}
ptm <- proc.time()
pars <- replicate(1000, optim(rnorm(1,0,0.1), exp_u, gr=NULL, control=list(fnscale=-1), method="BFGS",
                              n=n, d_hat=d_hat, d_bar=d_bar, rho=rho, sig=sig, mu_0=mu_0, sd_0=sd_0)$par)
proc.time() - ptm

# Compare with previous optimisation
ptm <- proc.time()
pars <- replicate(10,get_opt_alpha(n, d_hat, d_bar, rho, sig, mu_m, mu_sd))
proc.time() - ptm

# Over 2000 times as fast
```

